---
title: "Data Science Final Project"
output:
  pdf_document: default
  html_notebook: default
author: Brandon Feder, Atticus Wang
date: May 30, 2022
---

# 1. Introduction

## Research question and hypothesis

Princeton has many diverse houses ranging in size, location, age, and many other factors. With this comes a huge range of house prices. But how do the characteristics of a house determine its price? Our research studies how factors such as the number of bedrooms, location, and the year built affect the price of the house. More precisely, we try to predict `soldPrice`, the price at which a house is sold, using the following possible predictors:

- nbhd (neighborhood) 
- bed (number of bedrooms) 
- fullBath (number of full baths) 
- halfBath (number of half baths) 
- style (style) 
- age (yearSold minus yearBuilt) 
- marketDays (days the house was on market) 
- yearSold, daySold, monthSold (date at which the house was sold)

Based off our own experience, we predict that neighborhood/address and the year built have a significant impact on the price of the house.

## Data description

Beatrice Bloom, a Princeton Residential Specialist, provides many great resources about the Princeton housing market including a table of houses sold in Princeton since 2011. This data can be found [here](https://www.realestate-princeton.com/market-analysis/princeton-pending/). We intend to use this data to answer our question. The data is stored in `./data/pton-market-data.csv` in the Github repo.

For a simple exploratory data analysis, we used `group_by` and `summarize` to find the top-10 styles and neighborhoods with the highest price. The results are shown in Table 1.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(glmnet)
library(caret)
house <- read.csv("./data/pton-market-data.csv")

#Tidying up

house <- house %>% 
  mutate(Price = strtoi(str_replace_all(str_sub(Sold.Price, 2, -4), ",", ""))) %>%
  rename(nbhd = Neighborhood, 
         bed = Bed.Rooms, 
         fullBath = Full.Baths, 
         halfBath = Half.Baths, 
         style = Style, 
         age = Year.Built) %>%
  rename(lotSize = Lot.Size, 
         lastPrice = Last.Price, 
         originalPrice = Original.Price, 
         soldPrice = Price) %>%
  select(- Sold.Price) %>%
  rename(marketDays = Days.on.Market) %>%
  separate(col = Sold.Date, into = c("monthSold", "daySold", "yearSold"), sep = "/") %>%
  mutate(yearSold = strtoi(yearSold) + 2000) %>%
  mutate(daySold = strtoi(daySold), monthSold = strtoi(monthSold)) %>%
  mutate(age = yearSold - strtoi(age)) %>%
  select(- Property.Marketing.Period) %>%
  mutate(originalPrice = strtoi(str_replace_all(str_sub(originalPrice, 2, -4), ",", ""))) %>%
  mutate(lastPrice = strtoi(str_replace_all(str_sub(lastPrice, 2, -4), ",", ""))) %>%
  select(- X) %>%
  mutate(lotSize = as.double(lotSize)) %>%
  select(- Address) %>%
  mutate(marketDays = strtoi(marketDays))

#some final tidying

badStyles <- house %>% 
  group_by(style) %>%
  summarise(count = n()) %>%
  arrange(count) %>%
  head(43)

house <- house %>%
  select(- lotSize) %>%     #too many NAs
  select(- originalPrice) %>%
  select(- lastPrice) %>%  #omit these two predictors because they are obviously way too similar to the response variable
  mutate(nbhd = replace(nbhd, nbhd == "Battelfield Area", "Battlefield Area"), 
         nbhd = replace(nbhd, nbhd == "princeton Ridge", "Princeton Ridge"), 
         nbhd = replace(nbhd, nbhd == "LIttlebrook", "Littlebrook"), 
         nbhd = replace(nbhd, nbhd == "griggs Farm", "Griggs Farm"), 
         nbhd = replace(nbhd, nbhd == "PrettyBrook Area", "Pretty Brook Area"), 
         nbhd = replace(nbhd, nbhd == "Palmer Sq", "Palmer Square"), 
         nbhd = replace(nbhd, nbhd == "Institute", "Institute Area"), 
         nbhd = replace(nbhd, nbhd == "Rriverside", "Riverside"), 
         nbhd = replace(nbhd, nbhd == "Rriverside", "Riverside"), 
         nbhd = replace(nbhd, nbhd == "Carnegie Lake", "Carnegie Lake Area"), 
         nbhd = replace(nbhd, nbhd == "Town", "Township")) %>%     #fix area names
  filter(! nbhd %in% c("Edgerstoune", "Northridge", "Rushbrook", "Russell Estates", 
                       "The Preserve", "Governors Lane", "Princeton Borough", 
                       "Preserve", "Queenston")) %>%  #delete areas with few houses
  filter(! style %in% badStyles$style) %>%  #delete styles with at most 3 houses
  mutate(style = replace(style, style == "Bi-level", "Bi-Level"), 
         style = replace(style, style == "End Unit", "End-Unit"), 
         style = replace(style, style == "Townhouse", "Townhome"))  #fix style names

knitr::kable(list(house %>% 
  group_by(style) %>%
  summarise(meanPrice = mean(soldPrice)) %>%
  arrange(desc(meanPrice)) %>%
  head(10),
  house %>% 
  group_by(nbhd) %>%
  summarise(meanPrice = mean(soldPrice)) %>%
  arrange(desc(meanPrice)) %>%
  head(10)), caption = "Top ten styles and neighborhoods with highest meanPrice")
```


# 2. Regression Analysis

## Description of models

The final model we adopted was Lasso (short for "Least Absolute Shrinkage and Selection Operator"), a generalization of usual linear regression. We chose this model because among all models we tried (see Table 2), Lasso has the lowest variance of errors (an RMSE of roughly 0.453 million dollars), and a decent R-squared value of 0.522.

Lasso tries to enhance prediction accuracy and model interpretability by performing both variable selection (selecting which predictors to take into account) and shrinkage (shrinking coefficients of less important predictors). For usual linear models, we often try to minimize the sum of squares $\sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{i,j}\beta_j)^2$. For Lasso, we impose a penalty for more complex models: what we minimize is the sum $\sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{i,j}\beta_j)^2 + \lambda \sum_{j=1}^p |\beta_j|$. Here, $\lambda \ge 0$ is a parameter that we can tune to best fit our scenario.

As usual, Lasso assumes that the response variable $y$ follows a linear relation with the predictor variables $y = X\beta + \epsilon$, where $\epsilon$ is Normally distributed with mean 0. Also, since all predictor variables are assumed to be quantitative, we transformed categorical variables such as `nbhd` into dummy variables.

```{r, echo = FALSE, warning = FALSE, message = FALSE} 
modelsPrint <- data.frame(R2 = c(0.5448, 0.5449, 0.5449, 0.517, 0.5227, 0.5192, 0.5996), RMSE = c(454579.0698, 454880.4127, 454880.4127, 455475.355, 452814.7395, 454296.0325, 507587.9640))
row.names(modelsPrint) <- c("full lm", "forward", "backward", "ridge", "lasso", "elastic net", "pcr")
knitr::kable(modelsPrint, caption = "Performance of all models we tried")
```

## Model output

We calculated the average R-squared and RMSE values using tenfold cross validation. The following graphs plot R-squared and RMSE values against lambda, the parameter in the Lasso model.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
house <- read.csv("./data/pton-market-data.csv")

#Tidying up

house <- house %>% 
  mutate(Price = strtoi(str_replace_all(str_sub(Sold.Price, 2, -4), ",", ""))) %>%
  rename(nbhd = Neighborhood, 
         bed = Bed.Rooms, 
         fullBath = Full.Baths, 
         halfBath = Half.Baths, 
         style = Style, 
         age = Year.Built) %>%
  rename(lotSize = Lot.Size, 
         lastPrice = Last.Price, 
         originalPrice = Original.Price, 
         soldPrice = Price) %>%
  select(- Sold.Price) %>%
  rename(marketDays = Days.on.Market) %>%
  separate(col = Sold.Date, into = c("monthSold", "daySold", "yearSold"), sep = "/") %>%
  mutate(yearSold = strtoi(yearSold) + 2000) %>%
  mutate(daySold = strtoi(daySold), monthSold = strtoi(monthSold)) %>%
  mutate(age = yearSold - strtoi(age)) %>%
  select(- Property.Marketing.Period) %>%
  mutate(originalPrice = strtoi(str_replace_all(str_sub(originalPrice, 2, -4), ",", ""))) %>%
  mutate(lastPrice = strtoi(str_replace_all(str_sub(lastPrice, 2, -4), ",", ""))) %>%
  select(- X) %>%
  mutate(lotSize = as.double(lotSize)) %>%
  select(- Address) %>%
  mutate(marketDays = strtoi(marketDays))

#some final tidying

badStyles <- house %>% 
  group_by(style) %>%
  summarise(count = n()) %>%
  arrange(count) %>%
  head(43)

house <- house %>%
  select(- lotSize) %>%     #too many NAs
  select(- originalPrice) %>%
  select(- lastPrice) %>%  #omit these two predictors because they are obviously way too similar to the response variable
  mutate(nbhd = replace(nbhd, nbhd == "Battelfield Area", "Battlefield Area"), 
         nbhd = replace(nbhd, nbhd == "princeton Ridge", "Princeton Ridge"), 
         nbhd = replace(nbhd, nbhd == "LIttlebrook", "Littlebrook"), 
         nbhd = replace(nbhd, nbhd == "griggs Farm", "Griggs Farm"), 
         nbhd = replace(nbhd, nbhd == "PrettyBrook Area", "Pretty Brook Area"), 
         nbhd = replace(nbhd, nbhd == "Palmer Sq", "Palmer Square"), 
         nbhd = replace(nbhd, nbhd == "Institute", "Institute Area"), 
         nbhd = replace(nbhd, nbhd == "Rriverside", "Riverside"), 
         nbhd = replace(nbhd, nbhd == "Rriverside", "Riverside"), 
         nbhd = replace(nbhd, nbhd == "Carnegie Lake", "Carnegie Lake Area"), 
         nbhd = replace(nbhd, nbhd == "Town", "Township")) %>%     #fix area names
  filter(! nbhd %in% c("Edgerstoune", "Northridge", "Rushbrook", "Russell Estates", 
                       "The Preserve", "Governors Lane", "Princeton Borough", 
                       "Preserve", "Queenston")) %>%  #delete areas with few houses
  filter(! style %in% badStyles$style) %>%  #delete styles with at most 3 houses
  mutate(style = replace(style, style == "Bi-level", "Bi-Level"), 
         style = replace(style, style == "End Unit", "End-Unit"), 
         style = replace(style, style == "Townhouse", "Townhome"))  #fix style names

house <- na.omit(house)



#normalize numerical data

houseNorm <- house %>%
  mutate_at(c("bed", "fullBath", "halfBath", "age", "monthSold", 
              "daySold", "yearSold", "marketDays"), ~(scale(.) %>% as.vector))

#divide data into 10 parts for cross-validation

set.seed(42)
houseSplit <- houseNorm
houseSplit$id <- sample(0:9, size = nrow(houseSplit), replace = TRUE)
# houseSplit %>%
#  group_by(id) %>%
#  summarize(count = n())

#define evaluation metrics: returns R-squared and RMS error. 

evalMetrics <- function(model, df, predictions, target){
    resids = df[,target] - predictions
    resids2 = resids**2
    N = length(predictions)
    r2 = round(summary(model)$r.squared, 3)
    adjR2 = round(summary(model)$adj.r.squared, 3)
    rmse = round(sqrt(sum(resids2)/N), 3)
    return(c(adjR2, rmse))
}

evalResults <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- round(1 - SSE / SST, 3)
  RMSE = round(sqrt(SSE/nrow(df)), 3)
  return(c(R_square, RMSE))
}

#performance prints two numbers: 
#the first one is the average R-squared across all 10 cross-validations
#the second one is the average RMSE (standard deviation of the ten RMS)

performance <- function(df, modelName) {
  pf <- data.frame(modelName(df, 0))
  for (i in 1:9) {
    pf <- cbind(pf, modelName(df, i))
  }
  pf <- t(pf)
  print(c(mean(pf[,1]), mean(pf[,2])))
  return(c(mean(pf[,1]), mean(pf[,2])))
}



#now we define the regression models!

#linear regression model using all predictors

linearModel <- function(df, testSet) {
  lr <- lm(soldPrice ~ ., data = select(filter(df, id != testSet), -id))
  summary(lr)
  predictions <- predict(lr, newdata = select(filter(df, id == testSet), -id))
  evalMetrics(lr, select(filter(df, id == testSet), -id), predictions, target = "soldPrice")
}

#linear regression with a subset of the predictors
#we use the step function for this

forwardModel <- function(df, testSet) {
  fitNone <- lm(soldPrice ~ 1 , data = select(filter(df, id != testSet), -id))
  forward <- step(fitNone, direction = "forward", trace = 0, 
                  scope = formula(lm(soldPrice ~ ., data = select(filter(df, id != testSet), -id))))
  predForward <- predict(forward, newdata = select(filter(df, id == testSet), -id))
  evalMetrics(forward, select(filter(df, id == testSet), -id), predForward, target = "soldPrice")
}

backwardModel <- function(df, testSet) {
  fitAll <- lm(soldPrice ~ ., data = select(filter(df, id != testSet), -id))
  backward <- step(fitAll, direction = "backward", trace = 0)
  predBackward <- predict(backward, newdata = select(filter(df, id == testSet), -id))
  evalMetrics(backward, select(filter(df, id == testSet), -id), predBackward, target = "soldPrice")
}

#performance(houseSplit, linearModel)
#performance(houseSplit, forwardModel)
#performance(houseSplit, backwardModel)
#all three models behave fairly similarly: R-Squared is acceptable, but there is a large variance

#we will use the glmnet package to build regularized regression models
#these include ridge, lasso, and elastic net

# install.packages("glmnet")
# install.packages("caret")
library(glmnet)
library(caret)

dummy <- as.data.frame(predict(dummyVars(soldPrice ~ ., data = house), newdata = house))
dummyNorm <- as.data.frame(scale(dummy))
dummySplit <- dummyNorm
dummySplit$id = houseSplit$id
#these don't have the response variable as a column!
dummySplitWithResponse <- dummySplit
dummySplitWithResponse$soldPrice = house$soldPrice

ridgeModel <- function(df, testSet) {
  x = as.matrix(select(filter(dummySplit, id != testSet), -id))
  xTest = as.matrix(select(filter(dummySplit, id == testSet), -id))
  y = filter(df, id != testSet)$soldPrice
  yTest = filter(df, id == testSet)$soldPrice
  cvModel <- cv.glmnet(x, y, alpha = 0)
  bestLambda <- cvModel$lambda.min
  bestModel <- glmnet(x, y, alpha = 0, lambda = bestLambda)
  predictions <- predict(bestModel, newx = xTest)
  evalResults(yTest, predictions, filter(df, id == testSet))
}

lassoModel <- function(df, testSet) {
  x = as.matrix(select(filter(dummySplit, id != testSet), -id))
  xTest = as.matrix(select(filter(dummySplit, id == testSet), -id))
  y = filter(df, id != testSet)$soldPrice
  yTest = filter(df, id == testSet)$soldPrice
  cvModel <- cv.glmnet(x, y, alpha = 1)
  bestLambda <- cvModel$lambda.min
  bestModel <- glmnet(x, y, alpha = 1, lambda = bestLambda)
  predictions <- predict(bestModel, newx = xTest)
  evalResults(yTest, predictions, filter(df, id == testSet))
}

#for the elastic net approach, we need to tune the parameters alpha and lambda
elasticNetModel <- function(df, testSet) {
  trCont <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "random", verboseIter = FALSE)
  elasticBest <- train(soldPrice ~ ., data = select(filter(df, id != testSet), -id), method = "glmnet",
                       preProcess = c("center", "scale"), tuneLength = 10, trControl = trCont)
  xTest = as.matrix(select(filter(dummySplit, id == testSet), -id))
  yTest = filter(houseSplit, id == testSet)$soldPrice
  predictions <- predict(elasticBest, xTest)
  evalResults(yTest, predictions, filter(houseSplit, id == testSet))
}

#performance(houseSplit, ridgeModel)
#the ridge model has roughly the same RMSE, but slightly lower R2
#performance(houseSplit, lassoModel)
#lasso has slightly lower RMSE, and a slightly higher R2 compared to ridge, but still lower than the linear model
#performance(dummySplitWithResponse, elasticNetModel)
#elastic net is between ridge and lasso in terms of both R2 and variance

#finally we try principal component regression (PCR)
#this is worth trying because many predictors are already correlated
#we can fine-tune the parameter ncomp, which has been set to 30 (half of the total number of predictors)

# install.packages("pls")
library(pls)

PCRModel <- function(df, testSet) {
  housePCR <- pcr(soldPrice ~ ., data = select(filter(df, id != testSet), -id), scale = TRUE, validation = "CV", ncomp = 30)
  predictions <- predict(housePCR, select(filter(df, id == testSet), -id), ncomp = 30)
  yTest <- filter(df, id == testSet)$soldPrice
  return(c(round(mean((predictions - yTest)^2)/mean((yTest - mean(yTest))^2),3), round(sqrt(sum((predictions - yTest)^2)/length(predictions)), 3)))
}

lassoPlot <- function(df, testSet, lambda) {
  x = as.matrix(select(filter(dummySplit, id != testSet), -id))
  xTest = as.matrix(select(filter(dummySplit, id == testSet), -id))
  y = filter(df, id != testSet)$soldPrice
  yTest = filter(df, id == testSet)$soldPrice
  model <- glmnet(x, y, alpha = 1, lambda = lambda)
  predictions <- predict(model, newx = xTest)
  evalResults(yTest, predictions, filter(df, id == testSet))
}

performancePlot <- function(df, modelName, parameter) {
  pf <- data.frame(modelName(df, 0, parameter))
  for (i in 1:9) {
    pf <- cbind(pf, modelName(df, i, parameter))
  }
  pf <- t(pf)
  return(list(parameter, mean(pf[,1]), mean(pf[,2])))
}

lassoPerformance <- data.frame(lambda = numeric(), R2 = numeric(), RMSE = numeric())
for (i in seq(-2, 4, by = 0.2)) {
  lassoPerformance[5*(i+2),] <- performancePlot(houseSplit, lassoPlot, 10^i)
}

lassoR2 <- as.data.frame(lassoPerformance) %>%
  ggplot(aes(x = log10(lambda), y = R2)) +
  geom_line()
lassoRMSE <- as.data.frame(lassoPerformance) %>%
  ggplot(aes(x = log10(lambda), y = RMSE)) +
  geom_line()
```

```{r show_figure, fig.width = 2.5, fig.height = 2.5, warning = FALSE, echo = FALSE}
lassoR2
lassoRMSE
```

As we can see, the best lambda occurs roughly at $10^{3.8} \approx 6309$. Using this lambda, we obtained the R-squared and RMSE values for Lasso in Table 2.

\newpage

## Interpretation of coefficients

Inspecting the coefficients chosen by the Lasso model, we found that among the total 61 variables (most of them dummy variables), only 41 of them have nonzero coefficients. The largest coefficients (in terms of absolute value) are `yearSold` (412690), followed by `fullBath` (232332) and `halfBath` (57121). This means that on average, house prices increase each year by around 0.4 million dollars, and each additional full bathroom increases house prices by around 0.23 million dollars.

```{r, warning = FALSE, echo = FALSE, results = "hide"}
x = as.matrix(select(filter(dummySplit, id != 0), -id))
xTest = as.matrix(select(filter(dummySplit, id == 0), -id))
y = filter(houseSplit, id != 0)$soldPrice
yTest = filter(houseSplit, id == 0)$soldPrice
bestLambda <- 6309
bestModel <- glmnet(x, y, alpha = 1, lambda = bestLambda)
bestModel$beta
```


# 3. Discussion and Limitations
Though our model fits the data well, we were restricted by the curse of dimensionality: a required sample size will grow exponentially with the number of dimensions of the data. That is, we may not have had sufficient data to guarantee without a reasonable doubt that our model did not “detect” a coincidence in the data that would not be prevalent with more data.

In addition, we did not have time to incorporate US census data as we discussed in our proposal. From personal experience, we believe that this data would not particularly insightful because “like” individuals tend to congregate in neighborhoods which were already analyzed in the data. That being said, incorporating this data is something that would be useful for a more in-depth analysis.

Finally, we should have adjusted house prices for inflation. The LASSO model shows that the year the house sold greatly affects the houses price. However, we are not sure if houses actually “age like wine,” or if they simply get more expensive inversely with the dollar’s relative worth.

# 4. Conclusion
Overall, our analysis was successful in that it answered the question: “What characteristics of a house in Princeton most greatly influences it’s price?” However, with more time we would have considered more characteristics of the house, attempted to use other models (maybe a neural network), and performed actual predictions based off our data.

# 5. Aditional Work

## Overview Of Regression Models Tested

The first model is the standard **multi-linear regression** we studied in class for the ladder half of the year. In summary, this model assumes the data can be modeled by a line and attempts to minimize the residuls, $\varepsilon$,  according to some metric related to the space the data resodes in. Often, the metric is the sum of the squares of these residuals.

**Stepwise regressions** (forward and backward) is a method of fitting regression models in which the choice of predictive variables an algorithm. In each step of this algorithm, a variable is considered for addition or subtraction from the set of explanatory variables based on some critera. The difference between the forward and backward regression is only the "direction" the model is constructed: In a foward regression, the variables that meet the given criteria are added while in a backward regression, the model initialy contains all the variables from which some are removed.

**Ridge regression** provides a possible solution to the imprecision of least square regressions when linear regression models have some multicollinear independent variables. More formaly, while the coefficient estimator in a least-squares linear regression is given by $$\hat \beta = (X^TX)^{-1}X^Ty$$ the ridge regression is given by $$\hat \beta (X^TX + kI_p)^{-1}X^Ty$$
where $I_p$ is the $p \times p$ multiplicative identiy and $k>0$ is a small.

Once again, LASSO tried to enhance prediction accuracy and model interpretability by performing both variable selection and shrinkage (shrking coefficents of less-importnt predictors). While least squares multi-linear regressions minimize  $$\sum_{i=1}^n(y_i - \beta_0-\sum_{j=1}^p x_{i,j}\beta_j)^2,$$ LASSO minimizes $$\sum_{i=1}^n(y_i - \beta_0-\sum_{j=1}^p x_{i,j}\beta_j)^2 + \lambda_1 \sum_{j=1}^p |\beta_j|$$
For all $\lambda_1\in \mathbb{R}$.

**Elastic net** regression provides a compromize between ridge regressions and LASSO regressions by introducing a second $\lambda_2 \in \mathbb{R}$ such that the minimizing expression is  $$\sum_{i=1}^n(y_i - \beta_0-\sum_{j=1}^p x_{i,j}\beta_j)^2 + \lambda_1 \sum_{j=1}^p |\beta_j| + \lambda_2 \sum_{j=1}^p |\beta_j|^2$$
The penalty terms in the model's parameter space looks something like [this](https://www.oreilly.com/library/view/machine-learning-with/9781787121515/assets/03902148-aac8-4968-a384-3ac2c2180e21.png).

Finally, **principal component regression (PCR)** first performs a principal components analysis (PCA) on the predictors, and then regresses the response variable on those principal components. The principal components of a dataset are chosen inductively, with each new vector chosen in the direction orthogonal to all previous components that best fits the data. This is useful when the original predictors are linearly correlated with each other, because PCA reduces the number of predictors (simplifying the model) while eliminating those linear correlations that might negatively impact linear regression efficiency.
